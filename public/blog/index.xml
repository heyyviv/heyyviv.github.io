<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Vivek&#39;s Field Notes</title>
    <link>//localhost:1313/blog/</link>
    <description>Recent content in Blogs on Vivek&#39;s Field Notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    
    
    
    
    <lastBuildDate>Thu, 26 Jun 2025 16:53:53 +0530</lastBuildDate>
    
    
    <atom:link href="//localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About LLM part 1 </title>
      <link>//localhost:1313/blog/about-llm-part-1/</link>
      <pubDate>Thu, 26 Jun 2025 16:53:53 +0530</pubDate>
      <guid>//localhost:1313/blog/about-llm-part-1/</guid>
      <description>&lt;h1 id=&#34;root-mean-square-layer-normalization&#34;&gt;Root Mean Square Layer Normalization&lt;/h1&gt;&#xA;&lt;p&gt;Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network.&#xA;LayerNorm was widely accepted because it&amp;rsquo;s simplicity and no dependence  among training cases and it also handle variable length inputs unlike BatchNorm.&#xA;Unfortunately, the incorporation of LayerNorm raises computational overhead. Although this is negligible to small and shallow neural models with few normalization layers, this problem becomes severe when underlying networks grow larger and deeper. As a result, the efficiency gain from faster and more stable training (in terms of number of training steps) is counter-balanced by an  increased computational cost per training step, which diminishes the net efficiency.&#xA;One major feature of LayerNorm that is widely regarded as contributions to the stabilization is its  recentering invariance property.&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;//localhost:1313/rmsnorm_1.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;RMSNorm which only focuses on re-scaling invariance and regularizes the summed inputs simply according to the root mean square (RMS) statistic&#xA;RMS Norm Equation&#xA;$$&#xA;\mathrm{RMS} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} x_i^2}&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;A well-known explanation of the success of LayerNorm is its re-centering and re-scaling invariance&#xA;property. The former enables the model to be insensitive to shift noises on both inputs and weights,&#xA;and the latter keeps the output representations intact when both inputs and weights are randomly&#xA;scaled&lt;/p&gt;&#xA;&lt;h1 id=&#34;positional-encoding&#34;&gt;Positional Encoding&lt;/h1&gt;&#xA;&lt;p&gt;Desirable Properties&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Each position needs a unique encoding that remains consistent regardless of sequence length&lt;/li&gt;&#xA;&lt;li&gt;The relationship between positions should be mathematically simple. If we know the encoding for position p, it should be straightforward to compute the encoding for position p+k, making it easier for the model to learn positional patterns.&lt;/li&gt;&#xA;&lt;li&gt;It would be ideal if our positional encodings could be drawn from a deterministic process. This should allow the model to learn the mechanism behind our encoding scheme efficiently.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Drawbacks of absolute positonal encoding&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Don&amp;rsquo;t capture relative position between tokens&lt;/li&gt;&#xA;&lt;li&gt;While absolute positional encoding captures the positional information for a word, it does not capture the positional information for the entire sentence&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Rotary Positional Encoding is a type of position encoding that encodes absolute positional information with a rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation&lt;/p&gt;&#xA;&lt;p&gt;we&amp;rsquo;ve generated a separate positional encoding vector and added it to our token embedding prior to our Q, K and V projections. By adding the positional information directly to our token embedding, we are polluting the semantic information with the positional information.&#xA;$$&#xA;R(m\theta) =&#xA;\begin{bmatrix}&#xA;\cos(m\theta) &amp;amp; -\sin(m\theta) \&#xA;\sin(m\theta) &amp;amp; \cos(m\theta)&#xA;\end{bmatrix}&#xA;$$&lt;/p&gt;&#xA;</description>
    </item>
    <item>
      <title>Training_LLM</title>
      <link>//localhost:1313/blog/training_llm/</link>
      <pubDate>Fri, 23 May 2025 15:12:55 +0530</pubDate>
      <guid>//localhost:1313/blog/training_llm/</guid>
      <description>&lt;h1 id=&#34;training-on-one-gpu&#34;&gt;Training on One GPU&lt;/h1&gt;&#xA;&lt;p&gt;when a model trained, there are 3 phases&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A forward pass, which passes inputs through the model to yield its outputs&lt;/li&gt;&#xA;&lt;li&gt;A backward pass to compute the gradients&lt;/li&gt;&#xA;&lt;li&gt;An optimization step using the gradients to update the parameters&#xA;The batch size (bs) is one of the important hyperparameters for model training; it affects both model convergence and throughput.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;A small batch size can be useful early in training to quickly move through the training landscape to reach an optimal learning point. However, further along in the model training, small batch sizes will keep gradients noisy, and the model may not be able to converge to the most optimal final performance. At the other extreme, a large batch size, while giving very accurate gradient estimations, will tend to make less use of each training token, rendering convergence slower and potentially wasting compute resources.&lt;/p&gt;&#xA;&lt;p&gt;Batch size also affects the time it takes to train on a given text dataset: a small batch size will require more optimizer steps to train on the same amount of samples. Optimizer steps are costly (in compute time), and the total time to train will thus increase compared to using a larger batch size. That being said, note that the batch size can often be adjusted quite widely around the optimal batch size without major impact on the performance of the model - that is, the sensitivity of final model performance to the exact batch size value is usually rather low around the optimal batch size.&#xA;In the LLM pretraining community, batch sizes are commonly reported in terms of tokens rather than number of samples&#xA;bst = batch size tokens&#xA;bs = batch size&#xA;seq = model input sequence length&#xA;bst = bs * seq&#xA;Llama 1 was trained with a batch size of ~4M tokens for 1.4 trillion tokens, while DeepSeek was trained with a batch size of ~60M tokens for 14 trillion tokens.&lt;/p&gt;&#xA;&lt;p&gt;we couldn&amp;rsquo;t calculate exact memory usage by a model cuz&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CUDA kernels typically require 1-2 GB of GPU memory&lt;/li&gt;&#xA;&lt;li&gt;Some memory is used for buffers and intermediate results, and there&amp;rsquo;s some memory that can&amp;rsquo;t be used due to fragmentation.&#xA;We could face out-of-memory (OOM) issues when training this large models but why?&#xA;When training a neural network model, we store several items in memory:&lt;/li&gt;&#xA;&lt;li&gt;Model weights&lt;/li&gt;&#xA;&lt;li&gt;Model gradients&lt;/li&gt;&#xA;&lt;li&gt;Optimizer states&lt;/li&gt;&#xA;&lt;li&gt;Activations needed to compute the gradients&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;First the activations increase quickly as we do the forward pass, then during the backward pass the gradients build up, and as the backward pass propagates, the stored activations used to compute the gradients are progressively cleared. Finally, we perform optimization, during which we need all the gradients, and then update the optimizer states before we start the next forward pass.&lt;/p&gt;&#xA;&lt;p&gt;An interesting observation here is that memory usage is not static for a given model; rather, it scales linearly with the batch size and quadratically with the sequence length. This means the activation memory is the part that will blow up when we increase our batch size or train with longer sequences.&lt;/p&gt;&#xA;&lt;p&gt;These graphs tell a striking story: for short sequences (or small batch sizes), memory usage for activations is almost negligible, but from around 2-4k tokens they start to take up a significant amount of memory, while usage for parameters, gradients, and optimizer states (as we’ll discuss later) is roughly independent of the sequence length and batch size.&#xA;The general idea behind activation recomputation – also called gradient checkpointing or rematerialization – is to discard some activations during the forward pass to save memory and spend some extra compute to recompute these on the fly during the backward pass. Without recomputation, we store every hidden state between two learnable operations (e.g., feedforward, LayerNorm, etc.), so that we can use them during the backward pass to compute gradients. When we use recomputation, we typically only store activations at a few key points in the model architecture, discarding the rest of the activations and recomputing them on the fly during the backward pass from the nearest saved activations. Basically, we perform a sub-part of the forward pass again, to trade off memory for compute.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;FULL : We checkpoint activations at the transition point between each layer of the Transformer model. This is usually called the “full” strategy since it requires a forward pass through each layer, essentially adding a full forward pass during the backward pass. This strategy saves the most memory but is the most expensive one in terms of compute. It typically increases the compute cost and time by up to 30-40%, which is very noticeable.&lt;/li&gt;&#xA;&lt;li&gt;Selective: In general, we can do better than full. The authors of the recomputation paper did a detailed analysis studying which activations grow the largest and have the cheapest recomputation cost in terms of floating-point operations per second (FLOPS). It turns out that the attention computations fall in that category, and thus we can usually discard them and focus on checkpointing the expensive feedforward computations. For a GPT-3 (175B) model, this means a 70% activation memory reduction at a 2.7% compute cost.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Gradient accumulation is a very straightforward method to avoid memory explosion that consists of splitting a batch into micro-batches. We then perform forward and backward passes successively on each micro-batch, compute the gradients, and, as the name suggests, sum the gradients of all micro-batches before we perform optimization. In practice, the optimization step is conducted not on the sum but on the average of the gradients, so that the result is independent of the number of gradient accumulation steps.&#xA;Gradient accumulation allows us to reduce activation memory, which grows linearly with batch size, by processing smaller micro-batches sequentially. This reduces stored activations and gradients since only one micro-batch&amp;rsquo;s worth of activations needs to be kept in memory at a time, which helps reduce the overall activation memory footprint.&#xA;One drawback, however, is that gradient accumulation requires multiple consecutive forward/backward passes per optimization step, thereby increasing the compute overhead and slowing down training.&lt;/p&gt;&#xA;&lt;h1 id=&#34;data-parallelism&#34;&gt;Data Parallelism&lt;/h1&gt;&#xA;&lt;p&gt;The idea behind data parallelism (DP) is to replicate the model on several GPUs (we call the replicas “model instances”) and run forward and backward passes on different micro-batches of data in parallel on each GPU - hence the name data parallelism.&#xA;Using a different micro-batch for each GPU means we’ll have different gradients on each GPU, so to keep the model instances in sync across the different GPUs, we&amp;rsquo;ll average the gradients from the model instances using an operation called “all-reduce.” This operation takes place during the backward pass, before the optimizer step.&lt;/p&gt;&#xA;</description>
    </item>
    <item>
      <title>Hugo Shortcuts</title>
      <link>//localhost:1313/blog/hugo-shortcuts/</link>
      <pubDate>Thu, 22 May 2025 15:35:24 +0530</pubDate>
      <guid>//localhost:1313/blog/hugo-shortcuts/</guid>
      <description>&lt;h1 id=&#34;shortcuts&#34;&gt;Shortcuts&lt;/h1&gt;&#xA;&lt;h2 id=&#34;to-create-a-page&#34;&gt;to create a page&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;hugo new my-new-page.md&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;to-create-a-blog&#34;&gt;to create a blog&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;hugo new blog/my-new-post.md&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;run-locally&#34;&gt;run locally&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;hugo server&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This shortcuts will be used by me in future.&lt;/p&gt;&#xA;</description>
    </item>
  </channel>
</rss>