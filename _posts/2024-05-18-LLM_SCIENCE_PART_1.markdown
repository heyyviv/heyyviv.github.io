---
layout: post
title: "Kaggle - LLM Science Exam PART 1"
date: 2024-05-18 01:43:18 +0530
categories: Deep Learning,Kaggle
---

In this competition we are challenged to answer difficult science-based questions written by a Large Language Model.

The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.

An idea is to make this challenge a little easier by converting it to an open book science exam using semantic search and Wikipedia.

1.We obtain the plain text version of the latest dump from Wikipedia (https://www.kaggle.com/datasets/jjinho/wikipedia-20230701)
It was created using https://github.com/attardi/wikiextractor.

2.We will then convert the prompts into embeddings using sentence transformers (specifically using the all-MiniLM-L6-v2 model)

3.We will also create embeddings of all the Wikipedia articles, and to help us, use the first sentence from each article to provide more context (again using all-MiniLM-L6-v2)

4.We will then use faiss to perform similarity search to find the top-k articles that are most likely to have the information needed

5.We will then get the full text of those articles and split them into sentences using the fast blingfire package

6.Again, we will obtain embeddings of these sentences as well as embeddings of the prompt + answer choices and perform similarity search to get the top-k matching sentences for each question

7.We can then combine the questions, answer choices, and context to either perform straight up question answering, or feed into a LLM

Wikipedia 2023 07 faiss index
Embeddings of the title and first sentences of all articles from the 2023-07-01 dump of Wikipedia. Used the all-MiniLM-L6-v2 model from sentence_transformers to create the embedding

To use:

from faiss import write_index, read_index
sentence_index = read_index("wikipedia-2023-07-faiss-index/wikipedia_202307.index")

model = SentenceTransformer(MODEL, device=device)
model.max_seq_length = MAX_LENGTH
model = model.half()
MODEL we are using is sentence-transformers_all-MiniLM-L6-v2
MAX_LENGTH = 384
BATCH_SIZE = 16

train dataframe is like  [id	prompt	A	B	C	D	E	answer]

prompt is the question ,A-E is the option . answer is correct option

sentence_index = read_index("/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index")
contain 2 column  faiss-index and id number of wikipedia page

we then convert every promt from train dataframe to embedding using all-MiniLM-L6-v2

prompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=device, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True).half()
prompt_embeddings = prompt_embeddings.detach().cpu().numpy()

promt_embedding is embedding matrix for each prompt in train set

to find top matching embedding in wikipedia data for a prompt
search_score, search_index = sentence_index.search(prompt_embeddings, 3)
for each prompt we will get top 3 matching article score and index number
The search method in FAISS (Facebook AI Similarity Search) is used to find the nearest neighbors of a given query embedding within an index of embeddings. 

{% highlight python %}
## Get the article and associated file location using the index
wikipedia_file_data = []

for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):
    
    ## Get indices by score threshold
    #scr_idx = idx[np.where(scr <= 0.85)]
    scr_idx = idx
    _df = df.loc[scr_idx].copy()
    _df['prompt_id'] = i
    wikipedia_file_data.append(_df)
wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)
wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)

{% endhighlight %}


wikipedia_file_data.head
	id	prompt_id	file
0	1141	151	    a.parquet
1	11963992 185	a.parquet
2	1200	63	   a.parquet
3	1234	130	   a.parquet
4	1317	89	    a.parquet

{% highlight python %}
## Get the full text data
wiki_text_data = []

for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):
    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]
    _df = pd.read_parquet(f"{WIKI_PATH}/{file}", columns=['id', 'text'])

    _df = _df[_df['id'].isin(_id)]
    wiki_text_data.append(_df)
    _ = gc.collect()
wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)

{% endhighlight %}
_id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]: Retrieves a list of unique IDs associated with the current file.
_df = pd.read_parquet(f"{WIKI_PATH}/{file}", columns=['id', 'text']): Reads the Parquet file corresponding to the current file name, loading only the 'id' and 'text' columns into a dataframe _df.
_df = _df[_df['id'].isin(_id)]: Filters _df to retain only rows with IDs present in the _id list.
wiki_text_data.append(_df): Appends the filtered dataframe _df to the wiki_text_data list.

wiki_text_data.head()
	id	                        text
0	1550261	The American Petroleum Institute gravity, or A...
1	46674381	In mathematics and physics, acceleration is th...
2	424420	Accelerator physics is a branch of applied phy...
3	1234	Acoustic theory is a scientific field that rel...
4	68418053	Alan Louis Selman (April 2, 1941 â€“ January 22,...

{% highlight python %}
def process_documents(documents: Iterable[str],
                      document_ids: Iterable,
                      split_sentences: bool = True,
                      filter_len: int = 3,
                      disable_progress_bar: bool = False) -> pd.DataFrame:
    """
    Main helper function to process documents from the EMR.

    :param documents: Iterable containing documents which are strings
    :param document_ids: Iterable containing document unique identifiers
    :param document_type: String denoting the document type to be processed
    :param document_sections: List of sections for a given document type to process
    :param split_sentences: Flag to determine whether to further split sections into sentences
    :param filter_len: Minimum character length of a sentence (otherwise filter out)
    :param disable_progress_bar: Flag to disable tqdm progress bar
    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`
    """
    
    df = sectionize_documents(documents, document_ids, disable_progress_bar)
    print(df.head())
    if split_sentences:
        df = sentencize(df.text.values, 
                        df.document_id.values,
                        df.offset.values, 
                        filter_len, 
                        disable_progress_bar)
    print(df.head())
    return df


def sectionize_documents(documents: Iterable[str],
                         document_ids: Iterable,
                         disable_progress_bar: bool = False) -> pd.DataFrame:
    """
    Obtains the sections of the imaging reports and returns only the 
    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).

    :param documents: Iterable containing documents which are strings
    :param document_ids: Iterable containing document unique identifiers
    :param disable_progress_bar: Flag to disable tqdm progress bar
    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`
    """
    processed_documents = []
    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):
        row = {}
        text, start, end = (document, 0, len(document))
        row['document_id'] = document_id
        row['text'] = text
        row['offset'] = (start, end)

        processed_documents.append(row)

    _df = pd.DataFrame(processed_documents)
    if _df.shape[0] > 0:
        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)
    else:
        return _df


def sentencize(documents: Iterable[str],
               document_ids: Iterable,
               offsets: Iterable[tuple[int, int]],
               filter_len: int = 3,
               disable_progress_bar: bool = False) -> pd.DataFrame:
    """
    Split a document into sentences. Can be used with `sectionize_documents`
    to further split documents into more manageable pieces. Takes in offsets
    to ensure that after splitting, the sentences can be matched to the
    location in the original documents.

    :param documents: Iterable containing documents which are strings
    :param document_ids: Iterable containing document unique identifiers
    :param offsets: Iterable tuple of the start and end indices
    :param filter_len: Minimum character length of a sentence (otherwise filter out)
    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`
    """

    document_sentences = []
    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):
        try:
            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)
            for o in sentence_offsets:
                if o[1]-o[0] > filter_len:
                    sentence = document[o[0]:o[1]]
                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])
                    row = {}
                    row['document_id'] = document_id
                    row['text'] = sentence
                    row['offset'] = abs_offsets
                    document_sentences.append(row)
        except:
            continue
    return pd.DataFrame(document_sentences)

{% endhighlight %}

text_to_sentences_and_offsets function from the Bling Fire library (blingfire) to tokenize a document into sentences and obtain their character offsets within the original document
These functions together provide a flexible pipeline to process documents, including sectionizing and sentencizing. The process_documents function orchestrates the entire process, calling sectionize_documents and optionally sentencize based on the provided parameters. This modular approach allows for easy customization and reuse of individual processing steps.

{% highlight python %}

## Parameter to determine how many relevant sentences to include
NUM_SENTENCES_INCLUDE = 3

## List containing Question, Choices, Context
prompt_contexts = []

## List containing just Context
contexts = []
for r in trn.itertuples():
    prompt_context = ""

    prompt_id = r.id

    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values
    prompt_context += "Question: " + trn.prompt.iloc[prompt_id] + "\n"

    prompt_context += "Choices:\n"
    prompt_context += "(A) " + trn.A.iloc[prompt_id] + "\n"
    prompt_context += "(B) " + trn.B.iloc[prompt_id] + "\n"
    prompt_context += "(C) " + trn.C.iloc[prompt_id] + "\n"
    prompt_context += "(D) " + trn.D.iloc[prompt_id] + "\n"
    prompt_context += "(E) " + trn.E.iloc[prompt_id] + "\n"

    if prompt_indices.shape[0] > 0:
        prompt_context += "Context:\n"
        ## Per Prompt Index
        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], "Flat")
        prompt_index.add(wiki_data_embeddings[prompt_indices])

        context = ""
        
        ## Get the top matches
        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)
        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):
            ## Threshold on the score
            if _s < 2:
                context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + "\n"
        prompt_context += context
        
    contexts.append(context)
    prompt_contexts.append(prompt_context)
{% endhighlight %}

The faiss.index_factory function is used to create a new FAISS index with specified parameters. In this case, it's creating a flat index, which is a simple but efficient type of index for nearest neighbor search.
faiss.index_factory creates a new FAISS index with specified parameters. Here, it creates a flat index with the same dimensionality as the embeddings (wiki_data_embeddings) and adds the embeddings of the relevant documents (wiki_data_embeddings[prompt_indices]) to the index.
