+++
date = '2025-06-26T16:53:53+05:30'
title = 'About LLM part 1 '
tags = ["LLM","notes"]
+++
# Root Mean Square Layer Normalization
Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network.
LayerNorm was widely accepted because it's simplicity and no dependence  among training cases and it also handle variable length inputs unlike BatchNorm.
Unfortunately, the incorporation of LayerNorm raises computational overhead. Although this is negligible to small and shallow neural models with few normalization layers, this problem becomes severe when underlying networks grow larger and deeper. As a result, the efficiency gain from faster and more stable training (in terms of number of training steps) is counter-balanced by an  increased computational cost per training step, which diminishes the net efficiency.
One major feature of LayerNorm that is widely regarded as contributions to the stabilization is its  recentering invariance property.

{{< figure src="/rmsnorm_1.png">}}
RMSNorm which only focuses on re-scaling invariance and regularizes the summed inputs simply according to the root mean square (RMS) statistic
RMS Norm Equation
$$
\mathrm{RMS} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} x_i^2}
$$

A well-known explanation of the success of LayerNorm is its re-centering and re-scaling invariance
property. The former enables the model to be insensitive to shift noises on both inputs and weights,
and the latter keeps the output representations intact when both inputs and weights are randomly
scaled

# Positional Encoding

Desirable Properties
- Each position needs a unique encoding that remains consistent regardless of sequence length
- The relationship between positions should be mathematically simple. If we know the encoding for position p, it should be straightforward to compute the encoding for position p+k, making it easier for the model to learn positional patterns.
- It would be ideal if our positional encodings could be drawn from a deterministic process. This should allow the model to learn the mechanism behind our encoding scheme efficiently.

Drawbacks of absolute positonal encoding
- Don't capture relative position between tokens
- While absolute positional encoding captures the positional information for a word, it does not capture the positional information for the entire sentence

Rotary Positional Encoding is a type of position encoding that encodes absolute positional information with a rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation


we've generated a separate positional encoding vector and added it to our token embedding prior to our Q, K and V projections. By adding the positional information directly to our token embedding, we are polluting the semantic information with the positional information.
$$
R(m\theta) = 
\begin{bmatrix}
\cos(m\theta) & -\sin(m\theta) \\
\sin(m\theta) & \cos(m\theta)
\end{bmatrix}
$$
{{< figure src="/rope_2.png">}}
The challenge with this solution was that it works only for 2D. Hence, the authors came up with a solution that takes token pairs. This is why ROPE embeddings require dimensions of even length.

{{< figure src="/rope_1.png">}}
